<div class=WordSection1 style='layout-grid:15.6pt'>


<p class=MsoNormal align=center style='text-align:center'><a name="OLE_LINK2"></a><a
name="OLE_LINK1"><span style='mso-bookmark:OLE_LINK2'><span style='font-size:
36.0pt;font-family:华文隶书'>哈尔滨工业大学<span lang=EN-US><o:p></o:p></span></span></span></a></p>


<p class=MsoNormal align=center style='text-align:center;line-height:50.0pt;
mso-line-height-rule:exactly'><span style='mso-bookmark:OLE_LINK1'><span
style='mso-bookmark:OLE_LINK2'><b><span lang=EN-US style='font-size:26.0pt;
mso-fareast-font-family:黑体'>&lt;&lt;</span></b></span></span><span
style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b><span
style='font-size:26.0pt;font-family:黑体;mso-ascii-font-family:Calibri;
mso-hansi-font-family:Calibri'>模式识别与深度学习</span></b></span></span><span
style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b><span
lang=EN-US style='font-size:26.0pt;mso-fareast-font-family:黑体'>&gt;&gt;<o:p></o:p></span></b></span></span></p>
<p class=MsoNormal align=center style='text-align:center;line-height:50.0pt;
mso-line-height-rule:exactly'><span style='mso-bookmark:OLE_LINK1'><span
style='mso-bookmark:OLE_LINK2'><b><span style='font-size:26.0pt;font-family:
黑体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri'>实验</span></b></span></span><span
style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b><span
lang=EN-US style='font-size:26.0pt;mso-fareast-font-family:黑体'>5 </span></b></span></span><span
style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b><span
style='font-size:26.0pt;font-family:黑体;mso-ascii-font-family:Calibri;
mso-hansi-font-family:Calibri'>实验报告</span></b></span></span><span
style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b><span
lang=EN-US style='font-size:26.0pt;mso-fareast-font-family:黑体'><o:p></o:p></span></b></span></span></p>
<p class=MsoNormal align=center style='text-align:center;line-height:50.0pt;
mso-line-height-rule:exactly'><span style='mso-bookmark:OLE_LINK1'><span
style='mso-bookmark:OLE_LINK2'><b><span style='font-size:26.0pt;font-family:
黑体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri'>（2020</span></b></span></span><span
style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b><span
lang=EN-US style='font-size:26.0pt;mso-fareast-font-family:黑体'></span></b></span></span><span
style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b><span
style='font-size:26.0pt;font-family:黑体;mso-ascii-font-family:Calibri;
mso-hansi-font-family:Calibri'>春季学期）</span></b></span></span><span
style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b><span
lang=EN-US style='font-size:26.0pt;mso-fareast-font-family:黑体'><o:p></o:p></span></b></span></span></p>







<div align=center>
<table class=MsoNormalTable border=1 cellspacing=0 cellpadding=0
 style='border-collapse:collapse;border:none;mso-border-top-alt:1.5pt;
 mso-border-left-alt:.5pt;mso-border-bottom-alt:1.5pt;mso-border-right-alt:
 .5pt;mso-border-color-alt:windowtext;mso-border-style-alt:solid;mso-yfti-tbllook:
 480;mso-padding-alt:0cm 5.4pt 0cm 5.4pt;mso-border-insideh:.5pt solid windowtext;
 mso-border-insidev:.5pt solid windowtext'>
 <tr style='mso-yfti-irow:0;mso-yfti-firstrow:yes'>
  <td width=189 style='width:141.5pt;border:solid windowtext 1.0pt;border-top:
  solid windowtext 1.5pt;mso-border-alt:solid windowtext .5pt;mso-border-top-alt:
  solid windowtext 1.5pt;background:#CCECFF;padding:0cm 5.4pt 0cm 5.4pt'>
  <p class=a3 align=right style='text-align:right;text-indent:32.15pt'><span
  style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b
  style='mso-bidi-font-weight:normal'><span style='font-size:16.0pt;font-family:
  楷体_GB2312;mso-hansi-font-family:"Times New Roman"'>成员<span lang=EN-US
  style='color:black;mso-color-alt:windowtext'>1</span><span style='color:black;
  mso-color-alt:windowtext'>：</span><span lang=EN-US><o:p></o:p></span></span></b></span></span></p>
  </td>
  <span style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'></span></span>
  <td width=239 style='width:178.9pt;border-top:solid windowtext 1.5pt;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  mso-border-left-alt:solid windowtext .5pt;mso-border-alt:solid windowtext .5pt;
  mso-border-top-alt:solid windowtext 1.5pt;padding:0cm 5.4pt 0cm 5.4pt'>
  <p class=a3 style='text-align:justify;text-justify:inter-ideograph;
  text-indent:32.15pt'><span style='mso-bookmark:OLE_LINK1'><span
  style='mso-bookmark:OLE_LINK2'><b style='mso-bidi-font-weight:normal'><span
  lang=EN-US style='font-size:16.0pt;font-family:"Times New Roman",serif;
  mso-fareast-font-family:楷体_GB2312'>1171000520 </span></b></span></span><span
  style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><span
  class=GramE><b style='mso-bidi-font-weight:normal'><span style='font-size:
  16.0pt;font-family:楷体_GB2312;mso-ascii-font-family:"Times New Roman";
  mso-hansi-font-family:"Times New Roman"'>鲍克勤</span></b></span></span></span><span
  style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b
  style='mso-bidi-font-weight:normal'><span lang=EN-US style='font-size:16.0pt;
  font-family:"Times New Roman",serif;mso-fareast-font-family:楷体_GB2312'><o:p></o:p></span></b></span></span></p>
  </td>
  <span style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'></span></span>




































## 实验目标

- ´基于PyTorch实现生成对抗网络

  ´拟合给定分布

  ´要求可视化训练过程

## 实验环境

本机

操作系统：Windows

CPU：6核

RAM：16G 

GPU：NVIDIA GTX 1060

GPU RAM：6G

## 实验准备

### GAN

#### GAN原理

GAN主要分为两个网络一个是生成网络Generation，另一个是辨别网络discimination

##### Gen

生成网络就是，模型通过一些数据，然后生成类似的数据。比如我们可以通过给机器一些动物照片，然后给定一个随机输入，让它根据这个随机输入自己生成一个动物图像，这就是经典的生成网络，但是这个生成模型会有一个巨大的问题，就是它总是希望生成的和输入的结果尽可能像，但是这个比较是很难衡量的，同时另一个问题是有可能过拟合，导致输入任何数据都会生成一个一模一样的结果。

##### DIS

DIS是一个判别模型，通过将上述生成网络生成的结果通过这个判别网络，这个判别网络的作用就是识别输入是正常的输入，还是虚假的输入即由上面的Gen网络生成的输入，这个网络的作用就是尽可能识别出来Gen网络生成的输入，而Gen网络则是需要尽可能的骗过这个网络。

下面与课件中一样，以图像生成为例

首先我们知道真实图片集的分布$P_{data(x)}$，x是一个真实图片，可以想象成一个向量，这个向量的集合分布就是$P_{data}$， 我们 需要生成一些也在这个分布内的图片。

假设GEN网络生成的分布假设位$P_G(x;\theta)$, $\theta$看作是这个网络的超参数，我们希望的就是这个分布能尽可能地你和真实图片集的分布，根据最让似然法，我们就是要最大化生成模型中的似然

![[公式]](https://www.zhihu.com/equation?tex=L+%3D+%5Cprod_%7Bi%3D1%7D%5E%7Bm%7DP_G%28x%5Ei%3B%5Ctheta%29+)

这个问题就等价于

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D%0A%5Ctheta+%5E%2A+%26%3D+arg%5C+%5Cmax_%7B%5Ctheta%7D%5Cprod_%7Bi%3D1%7D%5E%7Bm%7DP_G%28x%5Ei%3B%5Ctheta%29+%5C%5C%0A%26%3Darg%5C+%5Cmax_%7B%5Ctheta%7D%5C+log%5Cprod_%7Bi%3D1%7D%5E%7Bm%7DP_G%28x%5Ei%3B%5Ctheta%29+%5C%5C%0A%26%3Darg%5C+%5Cmax_%7B%5Ctheta%7D+%5Csum_%7Bi%3D1%7D%5E%7Bm%7DlogP_G%28x%5Ei%3B%5Ctheta%29+%5C%5C%0A%26+%5Capprox+arg%5C+%5Cmax_%7B%5Ctheta%7D%5C+E_%7Bx%5Csim+P_%7Bdata%7D%7D%5BlogP_G%28x%3B%5Ctheta%29%5D+%5C%5C%0A%26+%3D+arg%5C+%5Cmax_%7B%5Ctheta%7D%5Cint_%7Bx%7D+P_%7Bdata%7D%28x%29logP_G%28x%3B%5Ctheta%29dx+-+%5Cint_%7Bx%7DP_%7Bdata%7D%28x%29logP_%7Bdata%7D%28x%29dx+%5C%5C%0A%26%3Darg%5C+%5Cmax_%7B%5Ctheta%7D%5Cint_%7Bx%7DP_%7Bdata%7D%28x%29%28logP_G%28x%3B%5Ctheta%29-logP_%7Bdata%7D%28x%29%29dx+%5C%5C%0A%26%3Darg%5C+%5Cmin_%7B%5Ctheta%7D%5Cint_%7Bx%7DP_%7Bdata%7D%28x%29log+%5Cfrac%7BP_%7Bdata%7D%28x%29%7D%7BP_G%28x%3B%5Ctheta%29%7Ddx+%5C%5C%0A%26%3Darg%5C+%5Cmin_%7B%5Ctheta%7D%5C+KL%28P_%7Bdata%7D%28x%29%7C%7CP_G%28x%3B%5Ctheta%29%29%0A%5Cend%7Balign%7D%0A+)



也就是我们前面说的，要找到一组产参数使得两个分布尽可能相似，我们知道神经网络本质上可以拟合任意函数，所以我们需要训练这样一个生成网络，这里先给出GAN的公式

![[公式]](https://www.zhihu.com/equation?tex=V%28G%2CD%29%3DE_%7Bx%5Csim+P_%7Bdata%7D%7D%5BlogD%28x%29%5D+%2B+E_%7Bx%5Csim+P_G%7D%5Blog%281-D%28x%29%29%5D)

这个公式表示了，固定G，max(V,G)表示两者之间的差异，然后找到一个G使得最大值最小

![[公式]](https://www.zhihu.com/equation?tex=G%5E%2A%3Darg%5C+%5Cmin_%7BG%7D%5C+%5Cmax_D%5C+V%28G%2CD%29)

表面上看这个的意思是，D要让这个式子尽可能的大，也就是对于x是真实分布中，D(x)要接近与1，对于x来自于生成的分布，D(x)要接近于0，然后G要让式子尽可能的小，让来自于生成分布中的x，D(x)尽可能的接近1

### WGAN

#### WGAN改进点

首先WGAN成功的解决了一下几点问题

- 彻底解决GAN训练不稳定的问题，不再需要小心平衡生成器和判别器的训练程度
- 基本解决了collapse mode的问题，确保了生成样本的多样性
- 训练过程中终于有一个像交叉熵、准确率这样的数值来指示训练的进程，这个数值越小代表GAN训练得越好，代表生成器产生的图像质量越高
- 以上一切好处不需要精心设计的网络架构，最简单的多层全连接网络就可以做到

具体的做法是

- 判别器最后一层去掉sigmoid
- 生成器和判别器的loss不取log
- 每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c
- 不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行

首先说明GAN的问题

1. 判别器越好，容易导致生成器梯度消失严重。WGAN论文前作中有一个结论，再最优判别器的情况下，最小化生成器的loss等价于最小化两者之间的JS散度，而由于两个分布之间几乎不可能有不可忽略的重叠，所以无论他们相距多元JS散度都是log2，导致梯度为0，梯度消失
2. 会出现collapse mode现象导致只拟合一部分分布

WGAN的解决方法

1. 提出了Wassertein距离，用这个距离来衡量两个分布之间的距离
2. 将二分类问题转化为回归问题，最小化距离，解决了梯度消失问题

### WGAN-GP

#### WGAN-GP 改进点

WGAN存在的问题

解决Lipschitz系数限制的时候过于粗暴，强制要求梯度在一定范围内，实验表明这回导致大部分梯度聚集在两端，是一个不好的现象

![](http://www.twistedwg.com/assets/img/WGAN/weight.png)

解决方法：

在损失函数中引入梯度惩罚项，论文中有两点需要注意，一部分就是这个梯度惩罚项![img](http://www.twistedwg.com/assets/img/WGAN/equation3.png)

另一部分就是，代码视线中的这个式子

![0?wx_fmt=png](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/iaTa8ut6HiawCUoIVNsXpWVcLibMiaesQkjxXRkEMfdOhct4IMySEVTUDg4AZuQwh0yYR89n5qbNwFzcot5QCSKP0g/0?wx_fmt=png)

该式子的原因是，计算梯度的时候需要对整个数据集做采样，但是取整个数据集显然是不现实的事情，所以我们可以对这个数据集的两部分做一个插值，来拟合整个数据集



## 实验过程

### 数据集读取和处理

直接使用loadmat库读取mat文件，取前7000个作为训练样本，后1192个作为测试样本

```python
def get_data(train_size=7000):
    data = loadmat("./points.mat")
    data = data['xx']
    np.random.shuffle(data)
    train_set = data[:train_size]
    test_set = data[train_size:]
    return train_set,test_set
```

### 网络结构实现

#### GEN

使用MLP网络结构, 其他的没有什么好说的，每个线性连接层后面加上一个RELU激活函数，GAN，WGAN，WGAN-GP之间没有区别

``` python
    def __init__(self, input_size):
        super(gen,self).__init__()
        self.gen = nn.Sequential(
            nn.Linear(input_size, 256),
            nn.ReLU(inplace=False),
            nn.Linear(256, 64),
            nn.ReLU(inplace=False),
            nn.Linear(64, 2)
        )
```

#### DIS

与GEN网络类似，同样是一个MLP网络结构，但是输出纬度时1

```python
def __init__(self):
    super(dis, self).__init__()
    self.dis = nn.Sequential(
        nn.Linear(2,256),
        nn.ReLU(inplace=False),
        nn.Linear(256,64),
        nn.ReLU(inplace=False),
        nn.Linear(64,1),
        nn.Sigmoid()
    )
```

需要注意的是在WGAN和WGAN-GP的时候，因为已经不再是一个二分类问题了，故需要将sigmoid删去

### 训练过程

#### GAN

两者均使用RMSprop优化器，学习率均设置为0.0001，训练200个epoch


#### WGAN

两者均使用RMSprop优化器，学习率均设置为0.0001，训练200个epoch

#### WGAN-GP

两者均使用RMSprop优化器，学习率设置为0.0001，训练200个epoch

训练过程中需要注意的是GAN的实现一般分为两种，一种是先更新判别器，再更新生成器，另一种是先更新生成器，再更新判别器，对于先更新判别器的计算图，网上说需要注意的是pytorch中，backward以后会清空 计算图，所以我们需要使用retain_graph=True保证计算图不被清空，但是我是用这种方法并没有成功，具体问题似乎是pytorch版本问题，所以我选用了第二种方法，先更新生成器，然后对生成器的输出使用detach函数即让它和再生成器的输出处断开连接更新判别器的时候不会再更新生成器。

``` python
            GEN_loss = -torch.mean(DIS(noise)).to(device)
            optimizer_g.zero_grad()
            GEN_loss.backward()
            optimizer_g.step()

            DIS_loss = -torch.mean(DIS(func) - DIS(noise.detach())).to(device) + 0.001* gradient_penalty(DIS, func, noise)
            optimizer_d.zero_grad()

            DIS_loss.backward()
            optimizer_d.step()
```





## 实验结果与分析

#### 稳定性与性能分析

GAN:200轮结果

![epoch200](D:\PatternRecognition\GAN\result\gan_rsp\epoch200.png)

WGAN 200轮结果

![epoch200](D:\PatternRecognition\GAN\result\wgan_rsp\epoch200.png)

WGAN-gp 200轮结果

![epoch200](D:\PatternRecognition\GAN\result\wgan_gp\epoch200.png)

可以看到变现的最好的显然是WGAN-gp，仅仅使用了200轮就完美拟合了原分布，其次是WGAN网络也做到了很好的拟合，最次是GAN网络也基本做到了拟合

从性能角度来说，在这个实验模型下面WGAN和GAN的变现相差不多，大概都是在70轮左右就已经对原模型做到了一定程度的拟合，WGAN-GP效果变现的最好，约30轮就做到了比较好的拟合。

从稳定性角度分析，WGAN-GP相比GAN和WGAN有比较大的提升，具体原因上述已经分析过了，WGAN-GP几乎解决了GAN和WGAN的所有问题，从理论上说WGAN解决了GAN部分问题应该比GAN表现优异才对，但是这次实验并没有明显体现出来，可能是参数选择问题，也可能是二维拟合比较简单

具体的变换过程在压缩包中的gan.gif,wgan.gif,wgan_gp.gif可以看到

#### SGD和RMSprop优化器对比

我使用GAN网络比较了SGD和RMSprop优化器

首先SGD200轮：

![epoch200](D:\PatternRecognition\GAN\result\gan\epoch200.png)

可以看到几乎没有拟合效果

RMS200轮

![epoch200](D:\PatternRecognition\GAN\result\gan_rsp\epoch200.png)

SGD2000轮

![epoch2000](D:\PatternRecognition\GAN\result\gan\epoch2000.png)

可以看到即使SGD2000轮以后的效果还是不如RMSprop200轮的效果，RMSprop优化器是基于SGD+动量优化器的一种优化，收敛效果更明显，且适用于WGAN和WGAN—GP

注意到根据论文中讲述WGAN并不适用Adam和SGD+动量算法



